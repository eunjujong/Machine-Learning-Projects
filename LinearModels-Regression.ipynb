{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from IPython.display import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear Regression over Boston Housing Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load Boston Housing dataset - goal, predict median home value (circa 1970)\n",
    "from sklearn.datasets import load_boston\n",
    "boston = load_boston()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate the training and test sets\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(boston.data, boston.target, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the package for Linear Regression\n",
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearRegression(copy_X=True, fit_intercept=True, n_jobs=None, normalize=False)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create a Linear Regression model\n",
    "lr = LinearRegression()\n",
    "# fit the Linear Regression model to the training data\n",
    "lr.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['CRIM' 'ZN' 'INDUS' 'CHAS' 'NOX' 'RM' 'AGE' 'DIS' 'RAD' 'TAX' 'PTRATIO'\n",
      " 'B' 'LSTAT']\n",
      "lr.coef_ [-1.17735289e-01  4.40174969e-02 -5.76814314e-03  2.39341594e+00\n",
      " -1.55894211e+01  3.76896770e+00 -7.03517828e-03 -1.43495641e+00\n",
      "  2.40081086e-01 -1.12972810e-02 -9.85546732e-01  8.44443453e-03\n",
      " -4.99116797e-01]\n",
      "lr.intercept_ 36.933255457118975\n",
      "3.7689676985862004 -15.589421129396712\n"
     ]
    }
   ],
   "source": [
    "# print the list of feature names for the boston data set\n",
    "print(boston.feature_names)\n",
    "# print the corresponding coefficient or \"weight\" for each input feature\n",
    "# Note: the _ after the name of an attribute of a model, like \"coef\", means that the\n",
    "# attribute is learned by the model, not entered as a parameter by the user\n",
    "print(\"lr.coef_\", lr.coef_)\n",
    "# print the intercept\n",
    "print(\"lr.intercept_\", lr.intercept_)\n",
    "print(max(lr.coef_), min(lr.coef_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### QUESTION: What do we observe that might be interesting when looking at the learned weights?\n",
    "\n",
    "CRIM: Per capita crime rate by town\n",
    "ZN: Proportion of residential land zoned for lots over 25,000 sq. ft\n",
    "INDUS: Proportion of non-retail business acres per town\n",
    "CHAS: Charles River dummy variable (= 1 if tract bounds river; 0 otherwise)\n",
    "NOX: Nitric oxide concentration (parts per 10 million)\n",
    "RM: Average number of rooms per dwelling\n",
    "AGE: Proportion of owner-occupied units built prior to 1940\n",
    "DIS: Weighted distances to five Boston employment centers\n",
    "RAD: Index of accessibility to radial highways\n",
    "TAX: Full-value property tax rate per $10,000\n",
    "PTRATIO: Pupil-teacher ratio by town\n",
    "B: 1000(Bk — 0.63)², where Bk is the proportion of [people of African American descent] by town\n",
    "LSTAT: Percentage of lower status of the population\n",
    "\n",
    "Q1: The positive coefficients of ZN, CHAS, RM, RAD and the negative coefficients of CRIM, INDUS, NOX, AGE, DIS, TAX, PTRATIO, and LSTAT make sense to me. Though I would not completely agree to the weigh of B, considering the weight of LSTAT, the weight of B can be explained. It is interesting to see that NOX affects the housing price the most (coeff. = -15.589)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set score: 0.770\n"
     ]
    }
   ],
   "source": [
    "# score our linear regression model against the training set\n",
    "print(\"Training set score: {:.3f}\".format(lr.score(X_train, y_train)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set score: 0.635\n"
     ]
    }
   ],
   "source": [
    "# score our linear regression model against the test set\n",
    "print(\"Test set score: {:.3f}\".format(lr.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### QUESTION: How does this result compared to kNN?\n",
    "Q2: Compared to the result of the kNN regressor with the best k value, the training score is a bit less, but close to the training score of kNN regressor (0.770 < 0.782). The testing score is greater for the linear regression model (0.635 > 0.510). ---> greater testing accuracy and less overfitting?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OBSERVATION: Learning will not succeed if your input data does not represent enough\n",
    "# information to generalize from."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Approach One for improvement: extend our feature set\n",
    "# Take the raw data in the Boston Housing Dataset and engineer a new dataset that includes\n",
    "# not just the originally collected features but also all products of all features\n",
    "# This lets the model take into account the interactions between features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(506, 13)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reminder: original dataset has 506 examples, 13 features per example\n",
    "boston.data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct expanded feature set\n",
    "from sklearn.preprocessing import MinMaxScaler, PolynomialFeatures\n",
    "expandedFeatures = MinMaxScaler().fit_transform(boston.data)\n",
    "expandedFeatures = PolynomialFeatures(degree=2, include_bias=False).fit_transform(expandedFeatures)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(506, 104)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# new feature set has 13 original features + 91 possible combinations of two features (with replacements)\n",
    "expandedFeatures.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(506,)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "boston.target.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new training and test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(expandedFeatures, boston.target, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearRegression(copy_X=True, fit_intercept=True, n_jobs=None, normalize=False)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create and fit a Linear Regression model to this data\n",
    "lr = LinearRegression()\n",
    "lr.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set score: 0.952\n",
      "Test set score: 0.607\n"
     ]
    }
   ],
   "source": [
    "# score our linear regression model against the training set and test set\n",
    "print(\"Training set score: {:.3f}\".format(lr.score(X_train, y_train)))\n",
    "print(\"Test set score: {:.3f}\".format(lr.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### QUESTION: Does our model learn better?\n",
    "Q3: The training score has increased, so the model learned better with the extended features. However, the testing accuracy has decreased. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OBSERVATION: When there is a large discrepancy between the training set performance and the\n",
    "# test set performance, it is a sign of overfitting - you are learning the training data well,\n",
    "# to the detriment of generalizing to new examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Approach Two for improvement: address overfitting by prefering models with small weights\n",
    "# Ridge Regression\n",
    "# A variation on Linear Regression, Ridge Regression learns weights for each feature that\n",
    "# both predict the training data but also are as close to zero as possible.\n",
    "# Controlling the weight values = regularization\n",
    "# Restricting a model in this way can often address overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import Ridge Regression\n",
    "from sklearn.linear_model import Ridge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Ridge(alpha=1.0, copy_X=True, fit_intercept=True, max_iter=None,\n",
       "      normalize=False, random_state=None, solver='auto', tol=0.001)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create the Ridge Regression model and fit it to the data\n",
    "ridge = Ridge();\n",
    "ridge.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set score: 0.886\n",
      "Test set score: 0.753\n"
     ]
    }
   ],
   "source": [
    "# score our ridge regression model against the training set and test set\n",
    "print(\"Training set score: {:.3f}\".format(ridge.score(X_train, y_train)))\n",
    "print(\"Test set score: {:.3f}\".format(ridge.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lr.coef_ [-4.12710947e+02 -5.22432068e+01 -1.31898815e+02 -1.20041365e+01\n",
      " -1.55107129e+01  2.87163342e+01  5.47040992e+01 -4.95346659e+01\n",
      "  2.65823927e+01  3.70620316e+01 -1.18281674e+01 -1.80581965e+01\n",
      " -1.95246830e+01  1.22025403e+01  2.98078144e+03  1.50084257e+03\n",
      "  1.14187325e+02 -1.69700520e+01  4.09613691e+01 -2.42636646e+01\n",
      "  5.76157466e+01  1.27812142e+03 -2.23986944e+03  2.22825472e+02\n",
      " -2.18201083e+00  4.29960320e+01 -1.33981515e+01 -1.93893485e+01\n",
      " -2.57541277e+00 -8.10130128e+01  9.66019367e+00  4.91423718e+00\n",
      " -8.12114800e-01 -7.64694179e+00  3.37837099e+01 -1.14464390e+01\n",
      "  6.85083979e+01 -1.73753604e+01  4.28128204e+01  1.13988209e+00\n",
      " -7.72696840e-01  5.68255921e+01  1.42875996e+01  5.39551110e+01\n",
      " -3.21709644e+01  1.92709675e+01 -1.38852338e+01  6.06343266e+01\n",
      " -1.23153942e+01 -1.20041365e+01 -1.77243899e+01 -3.39868183e+01\n",
      "  7.08999816e+00 -9.22538241e+00  1.71980268e+01 -1.27718431e+01\n",
      " -1.19727581e+01  5.73871915e+01 -1.75331865e+01  4.10103194e+00\n",
      "  2.93666477e+01 -1.76611772e+01  7.84049424e+01 -3.19098015e+01\n",
      "  4.81752461e+01 -3.95344813e+01  5.22959055e+00  2.19982410e+01\n",
      "  2.56483934e+01 -4.99982035e+01  2.91457545e+01  8.94267456e+00\n",
      " -7.16599297e+01 -2.28147862e+01  8.40660981e+00 -5.37905422e+00\n",
      "  1.20137322e+00 -5.20877186e+00  4.11452351e+01 -3.78250760e+01\n",
      " -2.67163851e+00 -2.55217108e+01 -3.33982030e+01  4.62272693e+01\n",
      " -2.41509169e+01 -1.77532970e+01 -1.39723701e+01 -2.35522208e+01\n",
      "  3.68353800e+01 -9.46890859e+01  1.44302810e+02 -1.51158659e+01\n",
      " -1.49513436e+01 -2.87729579e+01 -3.17673192e+01  2.49551594e+01\n",
      " -1.84384534e+01  3.65073948e+00  1.73101122e+00  3.53617137e+01\n",
      "  1.19553429e+01  6.77025947e-01  2.73452009e+00  3.03720012e+01]\n",
      "largest weight:  2980.7814393954595\n",
      "smallest weight:  -2239.869435560686\n"
     ]
    }
   ],
   "source": [
    "# inspect the learned weights for the linear regression model\n",
    "print(\"lr.coef_\", lr.coef_)\n",
    "print(\"largest weight: \", max(lr.coef_))\n",
    "print(\"smallest weight: \", min(lr.coef_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ridge.coef_ [-1.41368408e+00 -1.55661895e+00 -1.46543409e+00 -1.26616071e-01\n",
      " -7.91193605e-02  8.33161023e+00  2.54975060e-01 -4.94145701e+00\n",
      "  3.89862268e+00 -1.05866058e+00 -1.58433734e+00  1.05103856e+00\n",
      " -4.01220799e+00  3.33720475e-01  3.64725471e-03 -8.49295793e-01\n",
      "  7.44989267e-01 -1.43106836e+00 -1.62981017e+00 -1.40486294e+00\n",
      " -4.47314366e-02 -1.74619880e+00 -1.46715888e+00 -1.33237111e+00\n",
      " -1.69154625e+00 -5.06179637e-01  2.62197591e+00 -2.09210002e+00\n",
      "  1.95074661e-01 -2.75469422e-01  5.11308202e+00 -1.67083739e+00\n",
      " -9.81863179e-02  6.34477127e-01 -6.10008281e-01  4.01805897e-02\n",
      " -1.27661999e+00 -2.91349679e+00  3.39544035e+00  7.91904036e-01\n",
      "  1.35260232e+00 -4.03661265e+00  2.32361734e+00 -3.36712926e+00\n",
      "  1.81279204e+00  3.01566897e+00 -1.89452070e+00 -2.50844073e-01\n",
      " -2.89543735e+00 -1.26616071e-01 -5.00217192e+00 -2.43951806e+00\n",
      "  2.85071846e+00 -8.57081177e-01  2.99141960e+00  2.34589755e+00\n",
      "  1.31207081e+00  1.71845119e+00 -2.59766697e+00 -1.32370675e+00\n",
      " -2.81242223e+00 -2.09117058e+00 -1.08428335e+00 -2.73843625e+00\n",
      " -1.61989753e+00 -2.80493280e+00  9.44641482e-01 -1.65363374e+00\n",
      "  1.66553558e+01 -1.10980551e+00  2.14188605e+00 -8.03855387e+00\n",
      " -8.59149928e+00 -7.54161099e+00  1.02924022e+01 -7.96425897e+00\n",
      "  7.68540742e-01 -1.85213002e+00  2.51497387e+00 -3.42074257e-01\n",
      " -1.79604278e+00 -2.93048162e-01 -4.78242379e+00  8.63283317e-01\n",
      "  4.22361423e-01 -1.41656695e+00 -2.12023113e-01 -5.08121369e+00\n",
      " -5.47247509e-01  1.53835390e+00  1.81348033e+00  1.97252021e+00\n",
      "  1.81849652e+00 -7.14338697e+00  1.10472533e+00  1.42242216e+00\n",
      " -1.31494020e+00 -6.77170441e+00  1.82204476e+00 -2.36112444e+00\n",
      "  4.34670572e-02  1.20886000e+00 -6.32599163e+00  1.03600231e+01]\n",
      "largest weight:  16.655355821381683\n",
      "smallest weight:  -8.591499281027653\n"
     ]
    }
   ],
   "source": [
    "# inspect the learned weights for the ridge regression model\n",
    "print(\"ridge.coef_\", ridge.coef_)\n",
    "print(\"largest weight: \", max(ridge.coef_))\n",
    "print(\"smallest weight: \", min(ridge.coef_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### QUESTION: Did our model learn better? Are we overfitting? Are we generalizing?\n",
    "Q4: The training score of the ridge regression model is smaller than th etraining score of the extended feature lr model. So, the model is not learning better. However, the testing score increased and the discrepancy between the training score and the testing score is smaller. So, the risk of overfitting is less and the risk of generilization is less. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tuning our Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Like with kNN, we can provide Ridge Regression with a parameter for its model\n",
    "# Parameter indicates how strictly the model should prefer small weights over\n",
    "# fitting the training data\n",
    "# Larger alpha values indicate stronger preferences for small weights - MORE regularization\n",
    "# smaller alpha values indicate greater preference for fitting training data - LESS regularization\n",
    "# default alpha=1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set score: 0.788\n",
      "Test set score: 0.636\n"
     ]
    }
   ],
   "source": [
    "# Try Ridge Regression with an alpha value of 10\n",
    "ridge = Ridge(alpha=10);\n",
    "ridge.fit(X_train, y_train)\n",
    "print(\"Training set score: {:.3f}\".format(ridge.score(X_train, y_train)))\n",
    "print(\"Test set score: {:.3f}\".format(ridge.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set score: 0.928\n",
      "Test set score: 0.772\n"
     ]
    }
   ],
   "source": [
    "# Try Ridge Regression with an alpha of 0.1\n",
    "ridge = Ridge(alpha=0.1);\n",
    "ridge.fit(X_train, y_train)\n",
    "print(\"Training set score: {:.3f}\".format(ridge.score(X_train, y_train)))\n",
    "print(\"Test set score: {:.3f}\".format(ridge.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Experiment and try to find the best value for alpha that avoids overfitting\n",
    "# and yields the best performance on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0  Training set score: 0.952  Test set score: 0.604   0.34820975473289817\n",
      "0.04208416833667335  Training set score: 0.936  Test set score: 0.758   0.1782000548020476\n",
      "0.0841683366733467  Training set score: 0.930  Test set score: 0.770   0.15957074042748753\n",
      "0.12625250501002006  Training set score: 0.926  Test set score: 0.774   0.15177611712453398\n",
      "0.1683366733466934  Training set score: 0.922  Test set score: 0.775   0.14730691286532716\n",
      "0.21042084168336672  Training set score: 0.919  Test set score: 0.775   0.14434421845051437\n",
      "0.2525050100200401  Training set score: 0.916  Test set score: 0.774   0.1422099818881899\n",
      "0.29458917835671344  Training set score: 0.914  Test set score: 0.773   0.14058850150789348\n",
      "0.3366733466933868  Training set score: 0.912  Test set score: 0.772   0.13931084657083626\n",
      "0.3787575150300601  Training set score: 0.909  Test set score: 0.771   0.138277481597178\n",
      "0.42084168336673344  Training set score: 0.907  Test set score: 0.770   0.13742544236835907\n",
      "0.46292585170340683  Training set score: 0.905  Test set score: 0.769   0.13671266141044147\n",
      "0.5050100200400802  Training set score: 0.904  Test set score: 0.767   0.13610978638426163\n",
      "0.5470941883767535  Training set score: 0.902  Test set score: 0.766   0.13559560148734862\n",
      "0.5891783567134269  Training set score: 0.900  Test set score: 0.765   0.13515431713480208\n",
      "0.6312625250501002  Training set score: 0.898  Test set score: 0.764   0.13477388948296032\n",
      "0.6733466933867736  Training set score: 0.897  Test set score: 0.762   0.1344449371363503\n",
      "0.7154308617234469  Training set score: 0.895  Test set score: 0.761   0.13416001927174703\n",
      "0.7575150300601202  Training set score: 0.894  Test set score: 0.760   0.13391314064161075\n",
      "0.7995991983967936  Training set score: 0.892  Test set score: 0.759   0.13369940358377053\n",
      "0.8416833667334669  Training set score: 0.891  Test set score: 0.757   0.13351475795874967\n",
      "0.8837675350701403  Training set score: 0.889  Test set score: 0.756   0.13335581793450202\n",
      "0.9258517034068137  Training set score: 0.888  Test set score: 0.755   0.13321972540396854\n",
      "0.9679358717434869  Training set score: 0.887  Test set score: 0.754   0.13310404657154973\n",
      "1.0100200400801604  Training set score: 0.885  Test set score: 0.752   0.13300669254762332\n",
      "1.0521042084168337  Training set score: 0.884  Test set score: 0.751   0.13292585759705144\n",
      "1.094188376753507  Training set score: 0.883  Test set score: 0.750   0.13285997055698429\n",
      "1.1362725450901803  Training set score: 0.882  Test set score: 0.749   0.13280765620804602\n",
      "1.1783567134268538  Training set score: 0.881  Test set score: 0.748   0.13276770425914663\n",
      "1.220440881763527  Training set score: 0.880  Test set score: 0.747   0.1327390442207974\n",
      "1.2625250501002003  Training set score: 0.878  Test set score: 0.746   0.13272072487930675\n",
      "1.3046092184368738  Training set score: 0.877  Test set score: 0.745   0.1327118973998559\n",
      "1.346693386773547  Training set score: 0.876  Test set score: 0.744   0.1327118013169417\n",
      "1.3887775551102204  Training set score: 0.875  Test set score: 0.743   0.132719752841015\n",
      "1.4308617234468939  Training set score: 0.874  Test set score: 0.741   0.1327351350372883\n",
      "1.4729458917835672  Training set score: 0.873  Test set score: 0.740   0.13275738952864202\n",
      "1.5150300601202404  Training set score: 0.872  Test set score: 0.739   0.13278600944755226\n",
      "1.557114228456914  Training set score: 0.871  Test set score: 0.738   0.13282053341807787\n",
      "1.5991983967935872  Training set score: 0.870  Test set score: 0.737   0.13286054039233508\n",
      "1.6412825651302605  Training set score: 0.869  Test set score: 0.736   0.13290564519979553\n",
      "1.6833667334669338  Training set score: 0.868  Test set score: 0.735   0.13295549469435142\n",
      "1.7254509018036073  Training set score: 0.867  Test set score: 0.734   0.13300976440517864\n",
      "1.7675350701402806  Training set score: 0.867  Test set score: 0.734   0.13306815561420593\n",
      "1.8096192384769538  Training set score: 0.866  Test set score: 0.733   0.1331303927964469\n",
      "1.8517034068136273  Training set score: 0.865  Test set score: 0.732   0.13319622137029852\n",
      "1.8937875751503006  Training set score: 0.864  Test set score: 0.731   0.13326540571369627\n",
      "1.9358717434869739  Training set score: 0.863  Test set score: 0.730   0.1333377274091716\n",
      "1.9779559118236474  Training set score: 0.862  Test set score: 0.729   0.1334129836867275\n",
      "2.020040080160321  Training set score: 0.862  Test set score: 0.728   0.13349098603826892\n",
      "2.062124248496994  Training set score: 0.861  Test set score: 0.727   0.13357155898131223\n",
      "2.1042084168336674  Training set score: 0.860  Test set score: 0.726   0.13365453895301072\n",
      "2.1462925851703405  Training set score: 0.859  Test set score: 0.725   0.1337397733182908\n",
      "2.188376753507014  Training set score: 0.858  Test set score: 0.725   0.13382711947820525\n",
      "2.2304609218436875  Training set score: 0.858  Test set score: 0.724   0.13391644406655034\n",
      "2.2725450901803605  Training set score: 0.857  Test set score: 0.723   0.13400762222443252\n",
      "2.314629258517034  Training set score: 0.856  Test set score: 0.722   0.1341005369438476\n",
      "2.3567134268537075  Training set score: 0.855  Test set score: 0.721   0.13419507847252388\n",
      "2.3987975951903806  Training set score: 0.855  Test set score: 0.720   0.13429114377327545\n",
      "2.440881763527054  Training set score: 0.854  Test set score: 0.720   0.1343886360319666\n",
      "2.4829659318637276  Training set score: 0.853  Test set score: 0.719   0.13448746420892665\n",
      "2.5250501002004007  Training set score: 0.853  Test set score: 0.718   0.13458754262928774\n",
      "2.567134268537074  Training set score: 0.852  Test set score: 0.717   0.134688790608255\n",
      "2.6092184368737477  Training set score: 0.851  Test set score: 0.716   0.1347911321077968\n",
      "2.6513026052104207  Training set score: 0.851  Test set score: 0.716   0.13489449542164456\n",
      "2.693386773547094  Training set score: 0.850  Test set score: 0.715   0.13499881288585203\n",
      "2.7354709418837677  Training set score: 0.849  Test set score: 0.714   0.13510402061246607\n",
      "2.7775551102204408  Training set score: 0.849  Test set score: 0.713   0.13521005824413812\n",
      "2.8196392785571143  Training set score: 0.848  Test set score: 0.713   0.13531686872773407\n",
      "2.8617234468937878  Training set score: 0.847  Test set score: 0.712   0.13542439810521445\n",
      "2.903807615230461  Training set score: 0.847  Test set score: 0.711   0.13553259532023243\n",
      "2.9458917835671343  Training set score: 0.846  Test set score: 0.710   0.13564141203906266\n",
      "2.987975951903808  Training set score: 0.846  Test set score: 0.710   0.13575080248461535\n",
      "3.030060120240481  Training set score: 0.845  Test set score: 0.709   0.13586072328240606\n",
      "3.0721442885771544  Training set score: 0.844  Test set score: 0.708   0.1359711333174829\n",
      "3.114228456913828  Training set score: 0.844  Test set score: 0.708   0.13608199360138296\n",
      "3.156312625250501  Training set score: 0.843  Test set score: 0.707   0.13619326714830793\n",
      "3.1983967935871744  Training set score: 0.843  Test set score: 0.706   0.13630491885976348\n",
      "3.240480961923848  Training set score: 0.842  Test set score: 0.706   0.13641691541698697\n",
      "3.282565130260521  Training set score: 0.841  Test set score: 0.705   0.13652922518055144\n",
      "3.3246492985971945  Training set score: 0.841  Test set score: 0.704   0.13664181809658482\n",
      "3.3667334669338675  Training set score: 0.840  Test set score: 0.704   0.1367546656090991\n",
      "3.408817635270541  Training set score: 0.840  Test set score: 0.703   0.13686774057795892\n",
      "3.4509018036072145  Training set score: 0.839  Test set score: 0.702   0.13698101720207634\n",
      "3.4929859719438876  Training set score: 0.839  Test set score: 0.702   0.13709447094743743\n",
      "3.535070140280561  Training set score: 0.838  Test set score: 0.701   0.1372080784796097\n",
      "3.5771543086172346  Training set score: 0.838  Test set score: 0.700   0.1373218176004084\n",
      "3.6192384769539077  Training set score: 0.837  Test set score: 0.700   0.13743566718842026\n",
      "3.661322645290581  Training set score: 0.837  Test set score: 0.699   0.13754960714311815\n",
      "3.7034068136272547  Training set score: 0.836  Test set score: 0.698   0.13766361833230745\n",
      "3.7454909819639277  Training set score: 0.836  Test set score: 0.698   0.13777768254268574\n",
      "3.787575150300601  Training set score: 0.835  Test set score: 0.697   0.13789178243329037\n",
      "3.8296593186372747  Training set score: 0.835  Test set score: 0.697   0.1380059014916486\n",
      "3.8717434869739478  Training set score: 0.834  Test set score: 0.696   0.1381200239924445\n",
      "3.9138276553106213  Training set score: 0.834  Test set score: 0.695   0.1382341349585381\n",
      "3.9559118236472948  Training set score: 0.833  Test set score: 0.695   0.1383482201241818\n",
      "3.997995991983968  Training set score: 0.833  Test set score: 0.694   0.1384622659002922\n",
      "4.040080160320642  Training set score: 0.832  Test set score: 0.694   0.13857625934164508\n",
      "4.082164328657314  Training set score: 0.832  Test set score: 0.693   0.13869018811587164\n",
      "4.124248496993988  Training set score: 0.831  Test set score: 0.692   0.13880404047414185\n",
      "4.166332665330661  Training set score: 0.831  Test set score: 0.692   0.13891780522343133\n",
      "4.208416833667335  Training set score: 0.830  Test set score: 0.691   0.1390314717002712\n",
      "4.250501002004008  Training set score: 0.830  Test set score: 0.691   0.1391450297458936\n",
      "4.292585170340681  Training set score: 0.829  Test set score: 0.690   0.13925846968268596\n",
      "4.3346693386773545  Training set score: 0.829  Test set score: 0.690   0.13937178229187408\n",
      "4.376753507014028  Training set score: 0.829  Test set score: 0.689   0.13948495879236644\n",
      "4.4188376753507015  Training set score: 0.828  Test set score: 0.689   0.1395979908206828\n",
      "4.460921843687375  Training set score: 0.828  Test set score: 0.688   0.1397108704119121\n",
      "4.5030060120240485  Training set score: 0.827  Test set score: 0.687   0.13982358998163347\n",
      "4.545090180360721  Training set score: 0.827  Test set score: 0.687   0.1399361423087483\n",
      "4.587174348697395  Training set score: 0.826  Test set score: 0.686   0.14004852051916994\n",
      "4.629258517034068  Training set score: 0.826  Test set score: 0.686   0.14016071807032404\n",
      "4.671342685370742  Training set score: 0.826  Test set score: 0.685   0.14027272873641072\n",
      "4.713426853707415  Training set score: 0.825  Test set score: 0.685   0.14038454659439215\n",
      "4.755511022044089  Training set score: 0.825  Test set score: 0.684   0.1404961660106553\n",
      "4.797595190380761  Training set score: 0.824  Test set score: 0.684   0.1406075816283262\n",
      "4.839679358717435  Training set score: 0.824  Test set score: 0.683   0.1407187883551878\n",
      "4.881763527054108  Training set score: 0.824  Test set score: 0.683   0.14082978135217605\n",
      "4.923847695390782  Training set score: 0.823  Test set score: 0.682   0.1409405560224205\n",
      "4.965931863727455  Training set score: 0.823  Test set score: 0.682   0.14105110800080278\n",
      "5.008016032064129  Training set score: 0.822  Test set score: 0.681   0.14116143314400065\n",
      "5.050100200400801  Training set score: 0.822  Test set score: 0.681   0.1412715275209976\n",
      "5.092184368737475  Training set score: 0.822  Test set score: 0.680   0.1413813874040324\n",
      "5.134268537074148  Training set score: 0.821  Test set score: 0.680   0.14149100925996172\n",
      "5.176352705410822  Training set score: 0.821  Test set score: 0.679   0.14160038974201905\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.218436873747495  Training set score: 0.820  Test set score: 0.679   0.14170952568194983\n",
      "5.260521042084169  Training set score: 0.820  Test set score: 0.678   0.14181841408250007\n",
      "5.302605210420841  Training set score: 0.820  Test set score: 0.678   0.14192705211024248\n",
      "5.344689378757515  Training set score: 0.819  Test set score: 0.677   0.14203543708872668\n",
      "5.386773547094188  Training set score: 0.819  Test set score: 0.677   0.14214356649192916\n",
      "5.428857715430862  Training set score: 0.819  Test set score: 0.676   0.14225143793799755\n",
      "5.470941883767535  Training set score: 0.818  Test set score: 0.676   0.14235904918326747\n",
      "5.513026052104208  Training set score: 0.818  Test set score: 0.675   0.14246639811654294\n",
      "5.5551102204408815  Training set score: 0.817  Test set score: 0.675   0.14257348275362425\n",
      "5.597194388777555  Training set score: 0.817  Test set score: 0.674   0.14268030123207587\n",
      "5.6392785571142285  Training set score: 0.817  Test set score: 0.674   0.14278685180621786\n",
      "5.681362725450902  Training set score: 0.816  Test set score: 0.673   0.1428931328423333\n",
      "5.7234468937875755  Training set score: 0.816  Test set score: 0.673   0.14299914281408\n",
      "5.765531062124248  Training set score: 0.816  Test set score: 0.673   0.14310488029809898\n",
      "5.807615230460922  Training set score: 0.815  Test set score: 0.672   0.14321034396980659\n",
      "5.849699398797595  Training set score: 0.815  Test set score: 0.672   0.14331553259936647\n",
      "5.891783567134269  Training set score: 0.815  Test set score: 0.671   0.1434204450478287\n",
      "5.933867735470942  Training set score: 0.814  Test set score: 0.671   0.1435250802634317\n",
      "5.975951903807616  Training set score: 0.814  Test set score: 0.670   0.14362943727805766\n",
      "6.018036072144288  Training set score: 0.814  Test set score: 0.670   0.14373351520383382\n",
      "6.060120240480962  Training set score: 0.813  Test set score: 0.669   0.14383731322987603\n",
      "6.102204408817635  Training set score: 0.813  Test set score: 0.669   0.14394083061916485\n",
      "6.144288577154309  Training set score: 0.813  Test set score: 0.669   0.1440440667055496\n",
      "6.186372745490982  Training set score: 0.812  Test set score: 0.668   0.14414702089087517\n",
      "6.228456913827656  Training set score: 0.812  Test set score: 0.668   0.14424969264222498\n",
      "6.270541082164328  Training set score: 0.812  Test set score: 0.667   0.14435208148927625\n",
      "6.312625250501002  Training set score: 0.811  Test set score: 0.667   0.14445418702176172\n",
      "6.354709418837675  Training set score: 0.811  Test set score: 0.666   0.14455600888703235\n",
      "6.396793587174349  Training set score: 0.811  Test set score: 0.666   0.1446575467877197\n",
      "6.438877755511022  Training set score: 0.810  Test set score: 0.666   0.14475880047948864\n",
      "6.480961923847696  Training set score: 0.810  Test set score: 0.665   0.14485976976888315\n",
      "6.5230460921843685  Training set score: 0.810  Test set score: 0.665   0.14496045451125394\n",
      "6.565130260521042  Training set score: 0.809  Test set score: 0.664   0.14506085460877016\n",
      "6.6072144288577155  Training set score: 0.809  Test set score: 0.664   0.14516097000850825\n",
      "6.649298597194389  Training set score: 0.809  Test set score: 0.664   0.14526080070061698\n",
      "6.6913827655310625  Training set score: 0.808  Test set score: 0.663   0.14536034671655285\n",
      "6.733466933867735  Training set score: 0.808  Test set score: 0.663   0.14545960812738545\n",
      "6.775551102204409  Training set score: 0.808  Test set score: 0.662   0.14555858504216745\n",
      "6.817635270541082  Training set score: 0.808  Test set score: 0.662   0.1456572776063706\n",
      "6.859719438877756  Training set score: 0.807  Test set score: 0.662   0.14575568600037814\n",
      "6.901803607214429  Training set score: 0.807  Test set score: 0.661   0.14585381043803824\n",
      "6.943887775551103  Training set score: 0.807  Test set score: 0.661   0.14595165116527298\n",
      "6.985971943887775  Training set score: 0.806  Test set score: 0.660   0.14604920845873914\n",
      "7.028056112224449  Training set score: 0.806  Test set score: 0.660   0.14614648262454155\n",
      "7.070140280561122  Training set score: 0.806  Test set score: 0.660   0.14624347399699478\n",
      "7.112224448897796  Training set score: 0.806  Test set score: 0.659   0.146340182937433\n",
      "7.154308617234469  Training set score: 0.805  Test set score: 0.659   0.1464366098330634\n",
      "7.196392785571143  Training set score: 0.805  Test set score: 0.658   0.14653275509586483\n",
      "7.238476953907815  Training set score: 0.805  Test set score: 0.658   0.14662861916152659\n",
      "7.280561122244489  Training set score: 0.804  Test set score: 0.658   0.14672420248842877\n",
      "7.322645290581162  Training set score: 0.804  Test set score: 0.657   0.14681950555665868\n",
      "7.364729458917836  Training set score: 0.804  Test set score: 0.657   0.1469145288670678\n",
      "7.406813627254509  Training set score: 0.804  Test set score: 0.657   0.14700927294036037\n",
      "7.448897795591183  Training set score: 0.803  Test set score: 0.656   0.14710373831621826\n",
      "7.490981963927855  Training set score: 0.803  Test set score: 0.656   0.14719792555245836\n",
      "7.533066132264529  Training set score: 0.803  Test set score: 0.655   0.1472918352242204\n",
      "7.575150300601202  Training set score: 0.802  Test set score: 0.655   0.14738546792318596\n",
      "7.617234468937876  Training set score: 0.802  Test set score: 0.655   0.147478824256827\n",
      "7.659318637274549  Training set score: 0.802  Test set score: 0.654   0.14757190484768057\n",
      "7.701402805611223  Training set score: 0.802  Test set score: 0.654   0.14766471033265138\n",
      "7.7434869739478955  Training set score: 0.801  Test set score: 0.654   0.1477572413623408\n",
      "7.785571142284569  Training set score: 0.801  Test set score: 0.653   0.1478494986004001\n",
      "7.8276553106212425  Training set score: 0.801  Test set score: 0.653   0.14794148272290675\n",
      "7.869739478957916  Training set score: 0.800  Test set score: 0.652   0.14803319441776575\n",
      "7.9118236472945895  Training set score: 0.800  Test set score: 0.652   0.14812463438413026\n",
      "7.953907815631263  Training set score: 0.800  Test set score: 0.652   0.1482158033318468\n",
      "7.995991983967936  Training set score: 0.800  Test set score: 0.651   0.14830670198091822\n",
      "8.03807615230461  Training set score: 0.799  Test set score: 0.651   0.1483973310609874\n",
      "8.080160320641284  Training set score: 0.799  Test set score: 0.651   0.1484876913108404\n",
      "8.122244488977955  Training set score: 0.799  Test set score: 0.650   0.14857778347792638\n",
      "8.164328657314629  Training set score: 0.799  Test set score: 0.650   0.14866760831789794\n",
      "8.206412825651302  Training set score: 0.798  Test set score: 0.650   0.14875716659416427\n",
      "8.248496993987976  Training set score: 0.798  Test set score: 0.649   0.14884645907746452\n",
      "8.29058116232465  Training set score: 0.798  Test set score: 0.649   0.14893548654545463\n",
      "8.332665330661323  Training set score: 0.798  Test set score: 0.649   0.14902424978230988\n",
      "8.374749498997996  Training set score: 0.797  Test set score: 0.648   0.14911274957834253\n",
      "8.41683366733467  Training set score: 0.797  Test set score: 0.648   0.149200986729632\n",
      "8.458917835671343  Training set score: 0.797  Test set score: 0.648   0.1492889620376704\n",
      "8.501002004008017  Training set score: 0.797  Test set score: 0.647   0.14937667630901985\n",
      "8.54308617234469  Training set score: 0.796  Test set score: 0.647   0.14946413035498352\n",
      "8.585170340681362  Training set score: 0.796  Test set score: 0.647   0.14955132499128632\n",
      "8.627254509018035  Training set score: 0.796  Test set score: 0.646   0.14963826103777078\n",
      "8.669338677354709  Training set score: 0.796  Test set score: 0.646   0.14972493931810127\n",
      "8.711422845691382  Training set score: 0.795  Test set score: 0.646   0.14981136065948064\n",
      "8.753507014028056  Training set score: 0.795  Test set score: 0.645   0.14989752589237693\n",
      "8.79559118236473  Training set score: 0.795  Test set score: 0.645   0.14998343585025875\n",
      "8.837675350701403  Training set score: 0.795  Test set score: 0.645   0.15006909136934354\n",
      "8.879759519038076  Training set score: 0.794  Test set score: 0.644   0.1501544932883523\n",
      "8.92184368737475  Training set score: 0.794  Test set score: 0.644   0.15023964244827392\n",
      "8.963927855711423  Training set score: 0.794  Test set score: 0.644   0.15032453969213977\n",
      "9.006012024048097  Training set score: 0.794  Test set score: 0.643   0.1504091858648049\n",
      "9.04809619238477  Training set score: 0.793  Test set score: 0.643   0.1504935818127383\n",
      "9.090180360721442  Training set score: 0.793  Test set score: 0.643   0.15057772838382089\n",
      "9.132264529058116  Training set score: 0.793  Test set score: 0.642   0.15066162642715042\n",
      "9.17434869739479  Training set score: 0.793  Test set score: 0.642   0.15074527679285432\n",
      "9.216432865731463  Training set score: 0.793  Test set score: 0.642   0.15082868033191\n",
      "9.258517034068136  Training set score: 0.792  Test set score: 0.641   0.1509118378959705\n",
      "9.30060120240481  Training set score: 0.792  Test set score: 0.641   0.150994750337198\n",
      "9.342685370741483  Training set score: 0.792  Test set score: 0.641   0.15107741850810286\n",
      "9.384769539078157  Training set score: 0.792  Test set score: 0.640   0.1511598432613892\n",
      "9.42685370741483  Training set score: 0.791  Test set score: 0.640   0.15124202544980647\n",
      "9.468937875751504  Training set score: 0.791  Test set score: 0.640   0.15132396592600572\n",
      "9.511022044088177  Training set score: 0.791  Test set score: 0.639   0.15140566554240298\n",
      "9.553106212424849  Training set score: 0.791  Test set score: 0.639   0.15148712515104623\n",
      "9.595190380761522  Training set score: 0.790  Test set score: 0.639   0.15156834560348864\n",
      "9.637274549098196  Training set score: 0.790  Test set score: 0.639   0.15164932775066686\n",
      "9.67935871743487  Training set score: 0.790  Test set score: 0.638   0.1517300724427818\n",
      "9.721442885771543  Training set score: 0.790  Test set score: 0.638   0.1518105805291884\n",
      "9.763527054108216  Training set score: 0.790  Test set score: 0.638   0.15189085285828485\n",
      "9.80561122244489  Training set score: 0.789  Test set score: 0.637   0.15197089027740973\n",
      "9.847695390781563  Training set score: 0.789  Test set score: 0.637   0.15205069363274204\n",
      "9.889779559118237  Training set score: 0.789  Test set score: 0.637   0.15213026376920447\n",
      "9.93186372745491  Training set score: 0.789  Test set score: 0.636   0.15220960153037177\n",
      "9.973947895791584  Training set score: 0.788  Test set score: 0.636   0.15228870775838232\n",
      "10.016032064128257  Training set score: 0.788  Test set score: 0.636   0.15236758329385325\n",
      "10.05811623246493  Training set score: 0.788  Test set score: 0.636   0.15244622897579885\n",
      "10.100200400801603  Training set score: 0.788  Test set score: 0.635   0.15252464564155344\n",
      "10.142284569138276  Training set score: 0.788  Test set score: 0.635   0.1526028341266955\n",
      "10.18436873747495  Training set score: 0.787  Test set score: 0.635   0.15268079526497658\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10.226452905811623  Training set score: 0.787  Test set score: 0.634   0.1527585298882529\n",
      "10.268537074148297  Training set score: 0.787  Test set score: 0.634   0.15283603882641938\n",
      "10.31062124248497  Training set score: 0.787  Test set score: 0.634   0.15291332290734716\n",
      "10.352705410821644  Training set score: 0.786  Test set score: 0.633   0.15299038295682277\n",
      "10.394789579158317  Training set score: 0.786  Test set score: 0.633   0.1530672197984917\n",
      "10.43687374749499  Training set score: 0.786  Test set score: 0.633   0.1531438342538023\n",
      "10.478957915831664  Training set score: 0.786  Test set score: 0.633   0.15322022714195427\n",
      "10.521042084168338  Training set score: 0.786  Test set score: 0.632   0.1532963992798475\n",
      "10.56312625250501  Training set score: 0.785  Test set score: 0.632   0.15337235148203587\n",
      "10.605210420841683  Training set score: 0.785  Test set score: 0.632   0.1534480845606796\n",
      "10.647294589178356  Training set score: 0.785  Test set score: 0.631   0.1535235993255033\n",
      "10.68937875751503  Training set score: 0.785  Test set score: 0.631   0.15359889658375425\n",
      "10.731462925851703  Training set score: 0.785  Test set score: 0.631   0.1536739771401623\n",
      "10.773547094188377  Training set score: 0.784  Test set score: 0.631   0.15374884179690274\n",
      "10.81563126252505  Training set score: 0.784  Test set score: 0.630   0.15382349135356155\n",
      "10.857715430861724  Training set score: 0.784  Test set score: 0.630   0.15389792660710033\n",
      "10.899799599198397  Training set score: 0.784  Test set score: 0.630   0.15397214835182438\n",
      "10.94188376753507  Training set score: 0.784  Test set score: 0.629   0.154046157379353\n",
      "10.983967935871744  Training set score: 0.783  Test set score: 0.629   0.15411995447859084\n",
      "11.026052104208416  Training set score: 0.783  Test set score: 0.629   0.15419354043569822\n",
      "11.06813627254509  Training set score: 0.783  Test set score: 0.629   0.1542669160340694\n",
      "11.110220440881763  Training set score: 0.783  Test set score: 0.628   0.15434008205430472\n",
      "11.152304609218437  Training set score: 0.783  Test set score: 0.628   0.15441303927418915\n",
      "11.19438877755511  Training set score: 0.782  Test set score: 0.628   0.15448578846867256\n",
      "11.236472945891784  Training set score: 0.782  Test set score: 0.628   0.15455833040984535\n",
      "11.278557114228457  Training set score: 0.782  Test set score: 0.627   0.15463066586692376\n",
      "11.32064128256513  Training set score: 0.782  Test set score: 0.627   0.15470279560622935\n",
      "11.362725450901804  Training set score: 0.782  Test set score: 0.627   0.15477472039117424\n",
      "11.404809619238478  Training set score: 0.781  Test set score: 0.626   0.15484644098224443\n",
      "11.446893787575151  Training set score: 0.781  Test set score: 0.626   0.15491795813698628\n",
      "11.488977955911825  Training set score: 0.781  Test set score: 0.626   0.1549892726099933\n",
      "11.531062124248496  Training set score: 0.781  Test set score: 0.626   0.15506038515289333\n",
      "11.57314629258517  Training set score: 0.781  Test set score: 0.625   0.15513129651433832\n",
      "11.615230460921843  Training set score: 0.780  Test set score: 0.625   0.15520200743999324\n",
      "11.657314629258517  Training set score: 0.780  Test set score: 0.625   0.155272518672527\n",
      "11.69939879759519  Training set score: 0.780  Test set score: 0.625   0.15534283095160417\n",
      "11.741482965931864  Training set score: 0.780  Test set score: 0.624   0.15541294501387748\n",
      "11.783567134268537  Training set score: 0.780  Test set score: 0.624   0.15548286159298075\n",
      "11.82565130260521  Training set score: 0.779  Test set score: 0.624   0.15555258141952333\n",
      "11.867735470941884  Training set score: 0.779  Test set score: 0.624   0.15562210522108444\n",
      "11.909819639278558  Training set score: 0.779  Test set score: 0.623   0.15569143372220917\n",
      "11.951903807615231  Training set score: 0.779  Test set score: 0.623   0.1557605676444045\n",
      "11.993987975951905  Training set score: 0.779  Test set score: 0.623   0.1558295077061368\n",
      "12.036072144288577  Training set score: 0.778  Test set score: 0.622   0.15589825462282836\n",
      "12.07815631262525  Training set score: 0.778  Test set score: 0.622   0.15596680910685723\n",
      "12.120240480961924  Training set score: 0.778  Test set score: 0.622   0.15603517186755467\n",
      "12.162324649298597  Training set score: 0.778  Test set score: 0.622   0.15610334361120537\n",
      "12.20440881763527  Training set score: 0.778  Test set score: 0.621   0.15617132504104736\n",
      "12.246492985971944  Training set score: 0.777  Test set score: 0.621   0.15623911685727288\n",
      "12.288577154308618  Training set score: 0.777  Test set score: 0.621   0.15630671975702914\n",
      "12.330661322645291  Training set score: 0.777  Test set score: 0.621   0.1563741344344205\n",
      "12.372745490981965  Training set score: 0.777  Test set score: 0.620   0.15644136158050992\n",
      "12.414829659318638  Training set score: 0.777  Test set score: 0.620   0.15650840188332193\n",
      "12.456913827655312  Training set score: 0.777  Test set score: 0.620   0.15657525602784683\n",
      "12.498997995991983  Training set score: 0.776  Test set score: 0.620   0.15664192469604143\n",
      "12.541082164328657  Training set score: 0.776  Test set score: 0.619   0.15670840856683654\n",
      "12.58316633266533  Training set score: 0.776  Test set score: 0.619   0.15677470831613838\n",
      "12.625250501002004  Training set score: 0.776  Test set score: 0.619   0.15684082461683535\n",
      "12.667334669338677  Training set score: 0.776  Test set score: 0.619   0.15690675813880217\n",
      "12.70941883767535  Training set score: 0.775  Test set score: 0.618   0.15697250954890452\n",
      "12.751503006012024  Training set score: 0.775  Test set score: 0.618   0.1570380795110079\n",
      "12.793587174348698  Training set score: 0.775  Test set score: 0.618   0.15710346868598013\n",
      "12.835671342685371  Training set score: 0.775  Test set score: 0.618   0.1571686777317004\n",
      "12.877755511022045  Training set score: 0.775  Test set score: 0.617   0.15723370730306496\n",
      "12.919839679358718  Training set score: 0.774  Test set score: 0.617   0.15729855805199489\n",
      "12.961923847695392  Training set score: 0.774  Test set score: 0.617   0.1573632306274425\n",
      "13.004008016032063  Training set score: 0.774  Test set score: 0.617   0.1574277256754001\n",
      "13.046092184368737  Training set score: 0.774  Test set score: 0.616   0.15749204383890703\n",
      "13.08817635270541  Training set score: 0.774  Test set score: 0.616   0.15755618575805852\n",
      "13.130260521042084  Training set score: 0.774  Test set score: 0.616   0.15762015207001379\n",
      "13.172344689378757  Training set score: 0.773  Test set score: 0.616   0.15768394340900416\n",
      "13.214428857715431  Training set score: 0.773  Test set score: 0.615   0.15774756040634275\n",
      "13.256513026052104  Training set score: 0.773  Test set score: 0.615   0.15781100369043366\n",
      "13.298597194388778  Training set score: 0.773  Test set score: 0.615   0.15787427388677966\n",
      "13.340681362725451  Training set score: 0.773  Test set score: 0.615   0.15793737161799393\n",
      "13.382765531062125  Training set score: 0.773  Test set score: 0.615   0.1580002975038074\n",
      "13.424849699398798  Training set score: 0.772  Test set score: 0.614   0.1580630521610804\n",
      "13.46693386773547  Training set score: 0.772  Test set score: 0.614   0.15812563620381193\n",
      "13.509018036072144  Training set score: 0.772  Test set score: 0.614   0.15818805024314897\n",
      "13.551102204408817  Training set score: 0.772  Test set score: 0.614   0.15825029488739795\n",
      "13.59318637274549  Training set score: 0.772  Test set score: 0.613   0.15831237074203486\n",
      "13.635270541082164  Training set score: 0.771  Test set score: 0.613   0.15837427840971574\n",
      "13.677354709418838  Training set score: 0.771  Test set score: 0.613   0.15843601849028677\n",
      "13.719438877755511  Training set score: 0.771  Test set score: 0.613   0.15849759158079646\n",
      "13.761523046092185  Training set score: 0.771  Test set score: 0.612   0.15855899827550402\n",
      "13.803607214428858  Training set score: 0.771  Test set score: 0.612   0.15862023916589418\n",
      "13.845691382765532  Training set score: 0.771  Test set score: 0.612   0.15868131484068448\n",
      "13.887775551102205  Training set score: 0.770  Test set score: 0.612   0.15874222588583942\n",
      "13.929859719438879  Training set score: 0.770  Test set score: 0.611   0.15880297288457956\n",
      "13.97194388777555  Training set score: 0.770  Test set score: 0.611   0.15886355641739414\n",
      "14.014028056112224  Training set score: 0.770  Test set score: 0.611   0.15892397706205308\n",
      "14.056112224448897  Training set score: 0.770  Test set score: 0.611   0.15898423539361706\n",
      "14.098196392785571  Training set score: 0.770  Test set score: 0.611   0.15904433198444934\n",
      "14.140280561122244  Training set score: 0.769  Test set score: 0.610   0.15910426740422978\n",
      "14.182364729458918  Training set score: 0.769  Test set score: 0.610   0.15916404221996272\n",
      "14.224448897795591  Training set score: 0.769  Test set score: 0.610   0.15922365699599217\n",
      "14.266533066132265  Training set score: 0.769  Test set score: 0.610   0.15928311229401193\n",
      "14.308617234468938  Training set score: 0.769  Test set score: 0.609   0.15934240867307792\n",
      "14.350701402805612  Training set score: 0.769  Test set score: 0.609   0.15940154668962014\n",
      "14.392785571142285  Training set score: 0.768  Test set score: 0.609   0.15946052689745482\n",
      "14.434869739478959  Training set score: 0.768  Test set score: 0.609   0.15951934984779603\n",
      "14.47695390781563  Training set score: 0.768  Test set score: 0.608   0.15957801608926814\n",
      "14.519038076152304  Training set score: 0.768  Test set score: 0.608   0.15963652616791757\n",
      "14.561122244488978  Training set score: 0.768  Test set score: 0.608   0.1596948806272256\n",
      "14.603206412825651  Training set score: 0.768  Test set score: 0.608   0.15975308000811916\n",
      "14.645290581162325  Training set score: 0.767  Test set score: 0.608   0.15981112484898485\n",
      "14.687374749498998  Training set score: 0.767  Test set score: 0.607   0.15986901568567924\n",
      "14.729458917835672  Training set score: 0.767  Test set score: 0.607   0.15992675305154236\n",
      "14.771543086172345  Training set score: 0.767  Test set score: 0.607   0.15998433747740926\n",
      "14.813627254509019  Training set score: 0.767  Test set score: 0.607   0.16004176949162263\n",
      "14.855711422845692  Training set score: 0.767  Test set score: 0.606   0.1600990496200445\n",
      "14.897795591182366  Training set score: 0.766  Test set score: 0.606   0.16015617838606855\n",
      "14.939879759519037  Training set score: 0.766  Test set score: 0.606   0.16021315631063338\n",
      "14.98196392785571  Training set score: 0.766  Test set score: 0.606   0.16026998391223302\n",
      "15.024048096192384  Training set score: 0.766  Test set score: 0.606   0.16032666170693033\n",
      "15.066132264529058  Training set score: 0.766  Test set score: 0.605   0.16038319020836866\n",
      "15.108216432865731  Training set score: 0.766  Test set score: 0.605   0.1604395699277843\n",
      "15.150300601202405  Training set score: 0.765  Test set score: 0.605   0.16049580137401842\n",
      "15.192384769539078  Training set score: 0.765  Test set score: 0.605   0.1605518850535298\n",
      "15.234468937875752  Training set score: 0.765  Test set score: 0.604   0.1606078214704061\n",
      "15.276553106212425  Training set score: 0.765  Test set score: 0.604   0.16066361112637662\n",
      "15.318637274549099  Training set score: 0.765  Test set score: 0.604   0.1607192545208247\n",
      "15.360721442885772  Training set score: 0.765  Test set score: 0.604   0.1607747521507985\n",
      "15.402805611222446  Training set score: 0.764  Test set score: 0.604   0.16083010451102464\n",
      "15.444889779559118  Training set score: 0.764  Test set score: 0.603   0.16088531209391932\n",
      "15.486973947895791  Training set score: 0.764  Test set score: 0.603   0.16094037538960082\n",
      "15.529058116232465  Training set score: 0.764  Test set score: 0.603   0.16099529488590092\n",
      "15.571142284569138  Training set score: 0.764  Test set score: 0.603   0.16105007106837732\n",
      "15.613226452905812  Training set score: 0.764  Test set score: 0.603   0.16110470442032576\n",
      "15.655310621242485  Training set score: 0.763  Test set score: 0.602   0.1611591954227911\n",
      "15.697394789579159  Training set score: 0.763  Test set score: 0.602   0.1612135445545806\n",
      "15.739478957915832  Training set score: 0.763  Test set score: 0.602   0.16126775229227386\n",
      "15.781563126252506  Training set score: 0.763  Test set score: 0.602   0.16132181911023658\n",
      "15.823647294589179  Training set score: 0.763  Test set score: 0.601   0.16137574548063127\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15.865731462925853  Training set score: 0.763  Test set score: 0.601   0.1614295318734289\n",
      "15.907815631262526  Training set score: 0.763  Test set score: 0.601   0.16148317875642149\n",
      "15.949899799599198  Training set score: 0.762  Test set score: 0.601   0.16153668659523224\n",
      "15.991983967935871  Training set score: 0.762  Test set score: 0.601   0.16159005585332864\n",
      "16.034068136272545  Training set score: 0.762  Test set score: 0.600   0.1616432869920339\n",
      "16.07615230460922  Training set score: 0.762  Test set score: 0.600   0.1616963804705378\n",
      "16.118236472945892  Training set score: 0.762  Test set score: 0.600   0.16174933674590863\n",
      "16.160320641282567  Training set score: 0.762  Test set score: 0.600   0.16180215627310346\n",
      "16.20240480961924  Training set score: 0.761  Test set score: 0.600   0.16185483950498303\n",
      "16.24448897795591  Training set score: 0.761  Test set score: 0.599   0.1619073868923181\n",
      "16.286573146292586  Training set score: 0.761  Test set score: 0.599   0.1619597988838054\n",
      "16.328657314629258  Training set score: 0.761  Test set score: 0.599   0.16201207592607592\n",
      "16.370741482965933  Training set score: 0.761  Test set score: 0.599   0.1620642184637079\n",
      "16.412825651302605  Training set score: 0.761  Test set score: 0.599   0.16211622693923666\n",
      "16.45490981963928  Training set score: 0.760  Test set score: 0.598   0.1621681017931672\n",
      "16.49699398797595  Training set score: 0.760  Test set score: 0.598   0.16221984346398366\n",
      "16.539078156312627  Training set score: 0.760  Test set score: 0.598   0.1622714523881622\n",
      "16.5811623246493  Training set score: 0.760  Test set score: 0.598   0.16232292900017986\n",
      "16.623246492985974  Training set score: 0.760  Test set score: 0.597   0.1623742737325281\n",
      "16.665330661322646  Training set score: 0.760  Test set score: 0.597   0.16242548701572113\n",
      "16.707414829659317  Training set score: 0.760  Test set score: 0.597   0.162476569278309\n",
      "16.749498997995993  Training set score: 0.759  Test set score: 0.597   0.16252752094688705\n",
      "16.791583166332664  Training set score: 0.759  Test set score: 0.597   0.1625783424461067\n",
      "16.83366733466934  Training set score: 0.759  Test set score: 0.596   0.16262903419868668\n",
      "16.87575150300601  Training set score: 0.759  Test set score: 0.596   0.16267959662542375\n",
      "16.917835671342687  Training set score: 0.759  Test set score: 0.596   0.16273003014520282\n",
      "16.95991983967936  Training set score: 0.759  Test set score: 0.596   0.162780335175008\n",
      "17.002004008016034  Training set score: 0.759  Test set score: 0.596   0.16283051212993294\n",
      "17.044088176352705  Training set score: 0.758  Test set score: 0.595   0.16288056142319052\n",
      "17.08617234468938  Training set score: 0.758  Test set score: 0.595   0.16293048346612538\n",
      "17.128256513026052  Training set score: 0.758  Test set score: 0.595   0.1629802786682213\n",
      "17.170340681362724  Training set score: 0.758  Test set score: 0.595   0.16302994743711408\n",
      "17.2124248496994  Training set score: 0.758  Test set score: 0.595   0.1630794901786008\n",
      "17.25450901803607  Training set score: 0.758  Test set score: 0.594   0.16312890729664942\n",
      "17.296593186372746  Training set score: 0.757  Test set score: 0.594   0.1631781991934106\n",
      "17.338677354709418  Training set score: 0.757  Test set score: 0.594   0.16322736626922474\n",
      "17.380761523046093  Training set score: 0.757  Test set score: 0.594   0.1632764089226364\n",
      "17.422845691382765  Training set score: 0.757  Test set score: 0.594   0.16332532755040097\n",
      "17.46492985971944  Training set score: 0.757  Test set score: 0.593   0.1633741225474945\n",
      "17.507014028056112  Training set score: 0.757  Test set score: 0.593   0.16342279430712614\n",
      "17.549098196392787  Training set score: 0.757  Test set score: 0.593   0.16347134322074597\n",
      "17.59118236472946  Training set score: 0.756  Test set score: 0.593   0.16351976967805482\n",
      "17.63326653306613  Training set score: 0.756  Test set score: 0.593   0.16356807406701512\n",
      "17.675350701402806  Training set score: 0.756  Test set score: 0.592   0.16361625677385927\n",
      "17.717434869739478  Training set score: 0.756  Test set score: 0.592   0.1636643181831008\n",
      "17.759519038076153  Training set score: 0.756  Test set score: 0.592   0.16371225867754213\n",
      "17.801603206412825  Training set score: 0.756  Test set score: 0.592   0.16376007863828557\n",
      "17.8436873747495  Training set score: 0.756  Test set score: 0.592   0.16380777844474215\n",
      "17.88577154308617  Training set score: 0.755  Test set score: 0.592   0.16385535847464106\n",
      "17.927855711422847  Training set score: 0.755  Test set score: 0.591   0.16390281910403937\n",
      "17.96993987975952  Training set score: 0.755  Test set score: 0.591   0.16395016070732993\n",
      "18.012024048096194  Training set score: 0.755  Test set score: 0.591   0.16399738365725325\n",
      "18.054108216432866  Training set score: 0.755  Test set score: 0.591   0.16404448832490415\n",
      "18.09619238476954  Training set score: 0.755  Test set score: 0.591   0.1640914750797421\n",
      "18.138276553106213  Training set score: 0.755  Test set score: 0.590   0.1641383442896006\n",
      "18.180360721442884  Training set score: 0.754  Test set score: 0.590   0.16418509632069567\n",
      "18.22244488977956  Training set score: 0.754  Test set score: 0.590   0.16423173153763437\n",
      "18.26452905811623  Training set score: 0.754  Test set score: 0.590   0.1642782503034247\n",
      "18.306613226452907  Training set score: 0.754  Test set score: 0.590   0.16432465297948484\n",
      "18.34869739478958  Training set score: 0.754  Test set score: 0.589   0.16437093992565033\n",
      "18.390781563126254  Training set score: 0.754  Test set score: 0.589   0.1644171115001838\n",
      "18.432865731462925  Training set score: 0.753  Test set score: 0.589   0.16446316805978456\n",
      "18.4749498997996  Training set score: 0.753  Test set score: 0.589   0.16450910995959567\n",
      "18.517034068136272  Training set score: 0.753  Test set score: 0.589   0.16455493755321393\n",
      "18.559118236472948  Training set score: 0.753  Test set score: 0.588   0.1646006511926974\n",
      "18.60120240480962  Training set score: 0.753  Test set score: 0.588   0.16464625122857457\n",
      "18.64328657314629  Training set score: 0.753  Test set score: 0.588   0.16469173800985282\n",
      "18.685370741482966  Training set score: 0.753  Test set score: 0.588   0.16473711188402673\n",
      "18.727454909819638  Training set score: 0.752  Test set score: 0.588   0.16478237319708655\n",
      "18.769539078156313  Training set score: 0.752  Test set score: 0.588   0.16482752229352604\n",
      "18.811623246492985  Training set score: 0.752  Test set score: 0.587   0.1648725595163516\n",
      "18.85370741482966  Training set score: 0.752  Test set score: 0.587   0.16491748520709004\n",
      "18.895791583166332  Training set score: 0.752  Test set score: 0.587   0.16496229970579646\n",
      "18.937875751503007  Training set score: 0.752  Test set score: 0.587   0.165007003351063\n",
      "18.97995991983968  Training set score: 0.752  Test set score: 0.587   0.16505159648002654\n",
      "19.022044088176354  Training set score: 0.751  Test set score: 0.586   0.1650960794283769\n",
      "19.064128256513026  Training set score: 0.751  Test set score: 0.586   0.16514045253036502\n",
      "19.106212424849698  Training set score: 0.751  Test set score: 0.586   0.16518471611881014\n",
      "19.148296593186373  Training set score: 0.751  Test set score: 0.586   0.16522887052510893\n",
      "19.190380761523045  Training set score: 0.751  Test set score: 0.586   0.16527291607924188\n",
      "19.23246492985972  Training set score: 0.751  Test set score: 0.585   0.16531685310978228\n",
      "19.274549098196392  Training set score: 0.751  Test set score: 0.585   0.1653606819439033\n",
      "19.316633266533067  Training set score: 0.751  Test set score: 0.585   0.165404402907386\n",
      "19.35871743486974  Training set score: 0.750  Test set score: 0.585   0.16544801632462736\n",
      "19.400801603206414  Training set score: 0.750  Test set score: 0.585   0.16549152251864652\n",
      "19.442885771543086  Training set score: 0.750  Test set score: 0.585   0.1655349218110943\n",
      "19.48496993987976  Training set score: 0.750  Test set score: 0.584   0.16557821452225874\n",
      "19.527054108216433  Training set score: 0.750  Test set score: 0.584   0.16562140097107403\n",
      "19.569138276553108  Training set score: 0.750  Test set score: 0.584   0.16566448147512736\n",
      "19.61122244488978  Training set score: 0.750  Test set score: 0.584   0.16570745635066642\n",
      "19.65330661322645  Training set score: 0.749  Test set score: 0.584   0.16575032591260674\n",
      "19.695390781563127  Training set score: 0.749  Test set score: 0.583   0.1657930904745385\n",
      "19.7374749498998  Training set score: 0.749  Test set score: 0.583   0.1658357503487341\n",
      "19.779559118236474  Training set score: 0.749  Test set score: 0.583   0.16587830584615615\n",
      "19.821643286573146  Training set score: 0.749  Test set score: 0.583   0.16592075727646372\n",
      "19.86372745490982  Training set score: 0.749  Test set score: 0.583   0.1659631049480189\n",
      "19.905811623246493  Training set score: 0.749  Test set score: 0.583   0.16600534916789544\n",
      "19.947895791583168  Training set score: 0.748  Test set score: 0.582   0.1660474902418846\n",
      "19.98997995991984  Training set score: 0.748  Test set score: 0.582   0.16608952847450253\n",
      "20.032064128256515  Training set score: 0.748  Test set score: 0.582   0.16613146416899716\n",
      "20.074148296593187  Training set score: 0.748  Test set score: 0.582   0.16617329762735467\n",
      "20.11623246492986  Training set score: 0.748  Test set score: 0.582   0.1662150291503075\n",
      "20.158316633266534  Training set score: 0.748  Test set score: 0.581   0.16625665903733988\n",
      "20.200400801603205  Training set score: 0.748  Test set score: 0.581   0.16629818758669535\n",
      "20.24248496993988  Training set score: 0.747  Test set score: 0.581   0.16633961509538286\n",
      "20.284569138276552  Training set score: 0.747  Test set score: 0.581   0.16638094185918384\n",
      "20.326653306613228  Training set score: 0.747  Test set score: 0.581   0.16642216817265965\n",
      "20.3687374749499  Training set score: 0.747  Test set score: 0.581   0.16646329432915663\n",
      "20.410821643286575  Training set score: 0.747  Test set score: 0.580   0.16650432062081333\n",
      "20.452905811623246  Training set score: 0.747  Test set score: 0.580   0.16654524733856713\n",
      "20.49498997995992  Training set score: 0.747  Test set score: 0.580   0.16658607477216092\n",
      "20.537074148296593  Training set score: 0.747  Test set score: 0.580   0.16662680321014933\n",
      "20.579158316633265  Training set score: 0.746  Test set score: 0.580   0.16666743293990494\n",
      "20.62124248496994  Training set score: 0.746  Test set score: 0.580   0.1667079642476239\n",
      "20.663326653306612  Training set score: 0.746  Test set score: 0.579   0.1667483974183347\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20.705410821643287  Training set score: 0.746  Test set score: 0.579   0.16678873273590167\n",
      "20.74749498997996  Training set score: 0.746  Test set score: 0.579   0.1668289704830328\n",
      "20.789579158316634  Training set score: 0.746  Test set score: 0.579   0.16686911094128565\n",
      "20.831663326653306  Training set score: 0.746  Test set score: 0.579   0.16690915439107246\n",
      "20.87374749498998  Training set score: 0.745  Test set score: 0.579   0.16694910111166827\n",
      "20.915831663326653  Training set score: 0.745  Test set score: 0.578   0.16698895138121594\n",
      "20.95791583166333  Training set score: 0.745  Test set score: 0.578   0.167028705476731\n",
      "21.0  Training set score: 0.745  Test set score: 0.578   0.16706836367411004\n"
     ]
    }
   ],
   "source": [
    "alp = np.linspace(0,21,500)\n",
    "for i in alp: \n",
    "    ridge = Ridge(alpha=i);\n",
    "    ridge.fit(X_train, y_train)\n",
    "    print(i, \" Training set score: {:.3f}\".format(ridge.score(X_train, y_train)), \" Test set score: {:.3f}\".format(ridge.score(X_test, y_test)), \" \", ridge.score(X_train, y_train) - ridge.score(X_test, y_test))\n",
    "    #print(ridge.score(X_train, y_train) - ridge.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### QUESTION: What was the best alpha value found? How did you find it?\n",
    "Q5: When α = 1.3226, the testing score is 0.744 and the discrepancy between the training and testing scores is 0.13271. This selection is based on the mininum value of discrepancy between the training and testing scores. As α gets smaller, the performance of the prediction gets better. Though as α gets smaller, the discrepancy between the training and testing scores fluctuates, the discrepancy does not vary much (from 0.13271 to 0.1661). I tested the model on 500 α values between 1 and 20 (1 inclusive). ---> alp = np.linspace(0,20,500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
